{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification with Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multinomial_naive_bayes import MultinomialNaiveBayes\n",
    "from util import preprocess_data, get_dictionary, featurize_data, sign_test, cross_validation\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'datasets/data-tagged/'\n",
    "classes = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos, y_pos = preprocess_data(data_path, 'POS')\n",
    "X_neg, y_neg = preprocess_data(data_path, 'NEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_to_idx = get_dictionary(X_train, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "# X_train = featurize_data(X_train, token_to_idx)\n",
    "# X_test = featurize_data(X_test, token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model with Held Out Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = X_pos[:900] + X_neg[:900]\n",
    "y_train = y_pos[:900] + y_neg[:900]\n",
    "\n",
    "X_test = X_pos[900:] + X_neg[900:]\n",
    "y_test = y_pos[900:] + y_neg[900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing = 1\n",
    "unigram_cutoff = 1\n",
    "bigram_cutoff = 7\n",
    "unigram=True\n",
    "bigram=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 52555 features\n"
     ]
    }
   ],
   "source": [
    "token_to_idx = get_dictionary(X_train, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "X_feat_train = featurize_data(X_train, token_to_idx)\n",
    "X_feat_test = featurize_data(X_test, token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=smoothing)\n",
    "\n",
    "model.fit(X_feat_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.50% of sentences are correctly classified\n",
      "CPU times: user 151 ms, sys: 89.9 ms, total: 241 ms\n",
      "Wall time: 280 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = model.predict(X_feat_test)\n",
    "n_correct = sum(1 for i, _ in enumerate(y_pred) if y_pred[i] == y_test[i])\n",
    "\n",
    "print(\"{0:.2f}% of sentences are correctly classified\".format(n_correct * 100 / len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare models that use smoothing with models that don't use smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_cutoff = 4\n",
    "bigram_cutoff = 1\n",
    "unigram=True\n",
    "bigram=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 18333 features\n"
     ]
    }
   ],
   "source": [
    "token_to_idx = get_dictionary(X_train, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "X_feat_train = featurize_data(X_train, token_to_idx)\n",
    "X_feat_test = featurize_data(X_test, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ema/Projects/MPhil ACS/L90/multinomial_naive_bayes.py:38: RuntimeWarning: divide by zero encountered in log\n",
      "  log_cond_prob = np.log(self.class_to_feature_to_cond_prob[c])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: 0.0003943508751024046\n"
     ]
    }
   ],
   "source": [
    "model1 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=1)\n",
    "model2 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=0)\n",
    "\n",
    "# Train models on the same data\n",
    "model1.fit(X_feat_train, y_train)\n",
    "model2.fit(X_feat_train, y_train)\n",
    "\n",
    "# Test models\n",
    "y1_pred = model1.predict(X_feat_test)\n",
    "y2_pred = model2.predict(X_feat_test)\n",
    "\n",
    "sign_test(y1_pred, y2_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_cutoff = 1\n",
    "bigram_cutoff = 7\n",
    "unigram=False\n",
    "bigram=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 24990 features\n"
     ]
    }
   ],
   "source": [
    "token_to_idx = get_dictionary(X_train, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "X_feat_train = featurize_data(X_train, token_to_idx)\n",
    "X_feat_test = featurize_data(X_test, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ema/Projects/MPhil ACS/L90/multinomial_naive_bayes.py:38: RuntimeWarning: divide by zero encountered in log\n",
      "  log_cond_prob = np.log(self.class_to_feature_to_cond_prob[c])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: 0.04800146605383206\n"
     ]
    }
   ],
   "source": [
    "model1 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=1)\n",
    "model2 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=0)\n",
    "\n",
    "# Train models on the same data\n",
    "model1.fit(X_feat_train, y_train)\n",
    "model2.fit(X_feat_train, y_train)\n",
    "\n",
    "# Test models\n",
    "y1_pred = model1.predict(X_feat_test)\n",
    "y2_pred = model2.predict(X_feat_test)\n",
    "\n",
    "sign_test(y1_pred, y2_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams + Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_cutoff = 4\n",
    "bigram_cutoff = 7\n",
    "unigram=True\n",
    "bigram=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 43323 features\n"
     ]
    }
   ],
   "source": [
    "token_to_idx = get_dictionary(X_train, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "X_feat_train = featurize_data(X_train, token_to_idx)\n",
    "X_feat_test = featurize_data(X_test, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ema/Projects/MPhil ACS/L90/multinomial_naive_bayes.py:38: RuntimeWarning: divide by zero encountered in log\n",
      "  log_cond_prob = np.log(self.class_to_feature_to_cond_prob[c])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: 7.051394532266118e-05\n"
     ]
    }
   ],
   "source": [
    "model1 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=1)\n",
    "model2 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=0)\n",
    "\n",
    "# Train models on the same data\n",
    "model1.fit(X_feat_train, y_train)\n",
    "model2.fit(X_feat_train, y_train)\n",
    "\n",
    "# Test models\n",
    "y1_pred = model1.predict(X_feat_test)\n",
    "y2_pred = model2.predict(X_feat_test)\n",
    "\n",
    "sign_test(y1_pred, y2_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams vs Bigrams both with smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_cutoff = 4\n",
    "bigram_cutoff = 1\n",
    "unigram=True\n",
    "bigram=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 18333 features\n"
     ]
    }
   ],
   "source": [
    "token_to_idx = get_dictionary(X_train, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "X_feat_train = featurize_data(X_train, token_to_idx)\n",
    "X_feat_test = featurize_data(X_test, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=1)\n",
    "model1.fit(X_feat_train, y_train)\n",
    "y1_pred = model1.predict(X_feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_cutoff = 1\n",
    "bigram_cutoff = 7\n",
    "unigram=False\n",
    "bigram=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 24990 features\n"
     ]
    }
   ],
   "source": [
    "token_to_idx = get_dictionary(X_train, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "X_feat_train = featurize_data(X_train, token_to_idx)\n",
    "X_feat_test = featurize_data(X_test, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=1)\n",
    "model2.fit(X_feat_train, y_train)\n",
    "y2_pred = model2.predict(X_feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: 0.7779207003752835\n"
     ]
    }
   ],
   "source": [
    "sign_test(y1_pred, y2_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams+ Bigrams vs Unigrams both with smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_cutoff = 4\n",
    "bigram_cutoff = 7\n",
    "unigram=True\n",
    "bigram=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 43323 features\n"
     ]
    }
   ],
   "source": [
    "token_to_idx = get_dictionary(X_train, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "X_feat_train = featurize_data(X_train, token_to_idx)\n",
    "X_feat_test = featurize_data(X_test, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=1)\n",
    "model1.fit(X_feat_train, y_train)\n",
    "y1_pred = model1.predict(X_feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_cutoff = 4\n",
    "bigram_cutoff = 1\n",
    "unigram=True\n",
    "bigram=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 18333 features\n"
     ]
    }
   ],
   "source": [
    "token_to_idx = get_dictionary(X_train, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "X_feat_train = featurize_data(X_train, token_to_idx)\n",
    "X_feat_test = featurize_data(X_test, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=1)\n",
    "model2.fit(X_feat_train, y_train)\n",
    "y2_pred = model2.predict(X_feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: 0.5726756419095728\n"
     ]
    }
   ],
   "source": [
    "sign_test(y1_pred, y2_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X_pos + X_neg)\n",
    "y = np.array(y_pos + y_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_cutoff = 4\n",
    "bigram_cutoff = 1\n",
    "unigram=True\n",
    "bigram=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iteration 1 out of 10 of cross validation\n",
      "Generated 18403 features\n",
      "78.50% of sentences are correctly classified \n",
      "\n",
      "Running iteration 2 out of 10 of cross validation\n",
      "Generated 18309 features\n",
      "83.00% of sentences are correctly classified \n",
      "\n",
      "Running iteration 3 out of 10 of cross validation\n",
      "Generated 18353 features\n",
      "82.50% of sentences are correctly classified \n",
      "\n",
      "Running iteration 4 out of 10 of cross validation\n",
      "Generated 18338 features\n",
      "83.50% of sentences are correctly classified \n",
      "\n",
      "Running iteration 5 out of 10 of cross validation\n",
      "Generated 18350 features\n",
      "79.00% of sentences are correctly classified \n",
      "\n",
      "Running iteration 6 out of 10 of cross validation\n",
      "Generated 18285 features\n",
      "81.50% of sentences are correctly classified \n",
      "\n",
      "Running iteration 7 out of 10 of cross validation\n",
      "Generated 18265 features\n",
      "83.50% of sentences are correctly classified \n",
      "\n",
      "Running iteration 8 out of 10 of cross validation\n",
      "Generated 18367 features\n",
      "79.50% of sentences are correctly classified \n",
      "\n",
      "Running iteration 9 out of 10 of cross validation\n",
      "Generated 18398 features\n",
      "83.00% of sentences are correctly classified \n",
      "\n",
      "Running iteration 10 out of 10 of cross validation\n",
      "Generated 18406 features\n",
      "81.50% of sentences are correctly classified \n",
      "\n",
      "Finished running 10-fold cross validation\n",
      "Average number of features: 18347.4\n",
      "Accuracy is 81.55(+- 1.8090052515125543)\n",
      "\n",
      "Running iteration 1 out of 10 of cross validation\n",
      "Generated 18403 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ema/Projects/MPhil ACS/L90/multinomial_naive_bayes.py:38: RuntimeWarning: divide by zero encountered in log\n",
      "  log_cond_prob = np.log(self.class_to_feature_to_cond_prob[c])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.00% of sentences are correctly classified \n",
      "\n",
      "Running iteration 2 out of 10 of cross validation\n",
      "Generated 18309 features\n",
      "58.50% of sentences are correctly classified \n",
      "\n",
      "Running iteration 3 out of 10 of cross validation\n",
      "Generated 18353 features\n",
      "63.00% of sentences are correctly classified \n",
      "\n",
      "Running iteration 4 out of 10 of cross validation\n",
      "Generated 18338 features\n",
      "62.50% of sentences are correctly classified \n",
      "\n",
      "Running iteration 5 out of 10 of cross validation\n",
      "Generated 18350 features\n",
      "61.50% of sentences are correctly classified \n",
      "\n",
      "Running iteration 6 out of 10 of cross validation\n",
      "Generated 18285 features\n",
      "61.50% of sentences are correctly classified \n",
      "\n",
      "Running iteration 7 out of 10 of cross validation\n",
      "Generated 18265 features\n",
      "60.50% of sentences are correctly classified \n",
      "\n",
      "Running iteration 8 out of 10 of cross validation\n",
      "Generated 18367 features\n",
      "59.50% of sentences are correctly classified \n",
      "\n",
      "Running iteration 9 out of 10 of cross validation\n",
      "Generated 18398 features\n",
      "60.50% of sentences are correctly classified \n",
      "\n",
      "Running iteration 10 out of 10 of cross validation\n",
      "Generated 18406 features\n",
      "61.00% of sentences are correctly classified \n",
      "\n",
      "Finished running 10-fold cross validation\n",
      "Average number of features: 18347.4\n",
      "Accuracy is 60.55(+- 1.724093964956667)\n",
      "\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "integer division result too large for a float",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-84dc392f251e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my2_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munigram_cutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munigram_cutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigram_cutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbigram_cutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munigram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msign_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/MPhil ACS/L90/util.py\u001b[0m in \u001b[0;36msign_test\u001b[0;34m(y1_pred, y2_pred, y_test)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactorial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactorial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactorial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"p: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOverflowError\u001b[0m: integer division result too large for a float"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "model1 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=1)\n",
    "model2 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=0)\n",
    "\n",
    "y1_pred = cross_validation(model1, X, y, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "y2_pred = cross_validation(model2, X, y, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "sign_test(y1_pred, y2_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_to_idx = get_dictionary(X_train, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff, unigram=unigram, bigram=bigram)\n",
    "\n",
    "# X_feat_train = featurize_data(X_train, token_to_idx)\n",
    "# X_feat_test = featurize_data(X_test, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ema/Projects/MPhil ACS/L90/multinomial_naive_bayes.py:38: RuntimeWarning: divide by zero encountered in log\n",
      "  log_cond_prob = np.log(self.class_to_feature_to_cond_prob[c])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: 7.051394532266118e-05\n"
     ]
    }
   ],
   "source": [
    "model1 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=1)\n",
    "model2 = MultinomialNaiveBayes(classes, len(X_feat_train[0]), smoothing_value=0)\n",
    "\n",
    "# Train models on the same data\n",
    "model1.fit(X_feat_train, y_train)\n",
    "model2.fit(X_feat_train, y_train)\n",
    "\n",
    "# Test models\n",
    "y1_pred = model1.predict(X_feat_test)\n",
    "y2_pred = model2.predict(X_feat_test)\n",
    "\n",
    "sign_test(y1_pred, y2_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_cv():\n",
    "    smoothings = [0, 1]\n",
    "    unigrams = [True, False]\n",
    "    bigrams = [False, True]\n",
    "    unigram_cutoff = 4\n",
    "    bigram_cutoff = 7\n",
    "    \n",
    "    X = np.array(X_pos + X_neg)\n",
    "    y = np.array(y_pos + y_neg)\n",
    "    \n",
    "    for unigram in unigrams:\n",
    "        for bigram in bigrams:\n",
    "            for smoothing in smoothings:\n",
    "                if not unigram and not bigram:\n",
    "                    continue\n",
    "                print(\"unigram: {}, bigram: {}, unigram_cutoff: {}, bigram_cutoff: {}, smoothing: {}\".format(unigram, bigram, unigram_cutoff, bigram_cutoff, smoothing))\n",
    "\n",
    "                model = MultinomialNaiveBayes(classes, len(X_train[0]), smoothing_value=smoothing)\n",
    "                cross_validation(model, X, y, unigram=unigram, bigram=bigram, unigram_cutoff=unigram_cutoff, bigram_cutoff=bigram_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_full_cv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
